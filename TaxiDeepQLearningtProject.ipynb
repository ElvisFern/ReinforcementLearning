{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7GLdDUHu5Xx",
        "outputId": "3b7ca221-2179-4f46-b612-ef1bbfa1ea0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = []\n",
        "        self.gamma = 0.99    # discount rate\n",
        "        self.epsilon = 1.0   # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.1\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep Q-Learning\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(layers.Dense(24, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "      if len(self.memory) < batch_size:\n",
        "          return  0\n",
        "\n",
        "      minibatch_indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "      minibatch = [self.memory[i] for i in minibatch_indices]\n",
        "\n",
        "      losses=[]\n",
        "      for state, action, reward, next_state, done in minibatch:\n",
        "          target = reward\n",
        "          if not done:\n",
        "              target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "          target_f = self.model.predict(state)\n",
        "          target_f[0][action] = target\n",
        "          history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "          losses.append(history.history['loss'][0])\n",
        "      return np.mean(losses)  # Return the average loss\n",
        "\n",
        "def one_hot_state(state):\n",
        "    state_one_hot = np.zeros(state_size)\n",
        "    state_one_hot[state] = 1\n",
        "    return state_one_hot"
      ],
      "metadata": {
        "id": "HRP0nDl7vrHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5c4970-fdb8-4afe-97e2-8af0edaef34d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3',new_step_api=True)\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "episodes = 1000\n",
        "rewards_per_episode=[]\n",
        "losses = []\n",
        "\n",
        "\n",
        "for e in range(episodes):\n",
        "    total_episode_reward=0\n",
        "    state = env.reset()\n",
        "    state = one_hot_state(state)\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(500):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated  # Consider episode done if either flag is True\n",
        "        reward = reward if not done else -10\n",
        "        next_state = one_hot_state(next_state)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        # Rest of the loop\n",
        "\n",
        "        if done:\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, episodes, time, agent.epsilon))\n",
        "            break\n",
        "    rewards_per_episode.append(total_episode_reward)  # Store the total reward\n",
        "\n",
        "    if len(agent.memory) > 32:\n",
        "        loss = agent.replay(32)\n",
        "        losses.append(loss)  # Store the loss\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rewards_per_episode)\n",
        "plt.title('Training Rewards per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss per Replay')\n",
        "plt.xlabel('Replay')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RXzcJzyww2S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_episodes = 30\n",
        "test_rewards = []\n",
        "\n",
        "def one_hot_state(state, state_size):\n",
        "    state_one_hot = np.zeros((1, state_size))\n",
        "    state_one_hot[0, state] = 1\n",
        "    return state_one_hot\n",
        "\n",
        "for e in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    state = one_hot_state(state, state_size)\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(500):\n",
        "        action = np.argmax(agent.model.predict(state)[0])\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        next_state = one_hot_state(next_state, state_size)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    test_rewards.append(total_reward)\n",
        "\n",
        "average_test_reward = np.mean(test_rewards)\n",
        "print(f\"Average Test Reward: {average_test_reward}\")\n"
      ],
      "metadata": {
        "id": "d2A7VHa34okz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}